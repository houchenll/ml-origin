## 思想
卷积、全连接考虑的非随意线索，注意力机制考虑的是随意线索。  
随意线索称为`Query`，每个输入是一个非随意线索`key`和它的值`value`的对。  
通过注意力池化层来有偏向性地选择一些输入  
通过Q找到V里面比较重要的东西 


## 实现
给定一个与任务相关的查询向量 `Q` 以及备选元素`𝑥𝑖`，计算二者之间的相似度分数`𝑠(𝑄，𝑥𝑖)`。  
对计算得到的相似度分数`𝑠(𝑄，𝑥𝑖)`进行归一化，得到注意力权重`𝛼𝑖`。  
基于注意力机制的权重对备选元素的表示进行加权求和，得到最终的特征表示。`𝑄′ = ∑ 𝛼𝑖 𝑥𝑖`. 


## 自注意力机制
`Q`, `K`, `V`来源于同一个`X`，三者同源，Q $\approx$ K $\approx$ V。X经过三个线性变换，分别得到Q, K, V。其余操作和注意力机制一模一样。  
通过X找到X里面的关键点。
优点：
1. 解决RNN长序列依赖稀释问题
2. 解决RNN无法并行计算问题
3. 得到的词的新向量包含句法特征和语义特征，词向量表征更完善
缺点：
1. 计算量大。需要计算每个单词和所有单词的相似度，N^2级 

### 和注意力机制区别
注意力机制没有规定Q, K, V怎么来，

### 掩码自注意力机制

### 多头自注意力机制


## 交叉注意力机制
Q和V不同源，K和V同源


## 资料

### 博文
* [Attention 注意力机制简介](https://juejin.cn/post/7203285698074116151)

### 视频
* [64 注意力机制【动手学深度学习v2】](https://www.bilibili.com/video/BV1264y1i7R1/)
* [大白话浅谈【注意力机制】](https://www.bilibili.com/video/BV1Ff4y187nr/)
* [全网最透彻的注意力机制的通俗原理与本质](https://www.bilibili.com/video/BV1nL4y1j7hA/?spm_id_from=333.788&vd_source=7d7ddc7dc3515a96240c23fbbc06a03d)
* [注意力机制的本质|Self-Attention|Transformer|QKV矩阵](https://www.bilibili.com/video/BV1dt4y1J7ov/)
* [深度学习中注意力机制(attention)的真实由来](https://www.bilibili.com/video/BV1XT4y1y7P7/)
* [09 Transformer 之什么是注意力机制（Attention）](https://www.bilibili.com/video/BV1QW4y167iq)
* [10 Transformer 之 Self-Attention（自注意力机制）](https://www.bilibili.com/video/BV1ht4y187JE/)